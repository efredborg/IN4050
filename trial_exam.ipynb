{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hollow-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-empty",
   "metadata": {},
   "source": [
    "### Simulated annealing (6p)\n",
    "\n",
    "a) simulated annealing at very low temperature will be almost only exploitation, ergo similar to hill climbing.\n",
    "\n",
    "b) Holding the temperature very high will cause a lot of randomness and therefore not settle in the optimal areas in the landskape. This is similar to random search/exhaustive search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-taiwan",
   "metadata": {},
   "source": [
    "### Master student's only: search (5p)\n",
    "To impove a hill climbers chances of finding the global optimum I would initial the search in multiple random places making it more probable that one of the optima found is the global optimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-tamil",
   "metadata": {},
   "source": [
    "### Pareto optimality (9p)\n",
    "\n",
    "a) Solutions on the Pareto optimal set have to be non-dominated. That is, no other solutions should be better according to both objectives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-turkish",
   "metadata": {},
   "source": [
    "### Perceptron and linear regression classifier\n",
    "a) Showing that the data are clearly linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832f7bfc-2a4e-47e0-a11f-d44b78057b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(X_set, t_set):\n",
    "    t_unique = np.unique(t_set)\n",
    "    colors = cm.rainbow(np.linspace(0.0, 1.0, t_unique.size))\n",
    "    for this_t, color in zip(t_unique, colors):\n",
    "        this_X = X_set[t_set == this_t]\n",
    "        plt.scatter(this_X[:, 0], this_X[:, 1],\n",
    "                c=color[np.newaxis, :],\n",
    "                alpha=0.5, edgecolor='k',\n",
    "                label=\"Class %s\" % this_t)\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f87288-247f-4b1d-b13b-8473a4ed3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array((1,2,1,1))\n",
    "x2 = np.array((2,1,1,0))\n",
    "t = np.array((1,1,0,0))\n",
    "x = np.array((x1, x2))\n",
    "x = np.array(([[i,j] for i,j in zip(x1,x2)]))\n",
    "#print(x.shape)\n",
    "#plt.scatter(x[:,0], x[:,1], label = 'Class %s' % t)\n",
    "plt.figure(figsize = (6,6))\n",
    "show(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2577b-47b1-4878-9be0-7d834969ce2d",
   "metadata": {},
   "source": [
    "The data are clearly linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ebd07-69fd-47e7-affe-e1ea08addef4",
   "metadata": {},
   "source": [
    "b) We have $\\vec{w} = (0,-1,1)$ and learning rate $\\eta = 0.1$. We add bias $x_0 = -1$. \n",
    "\n",
    "$z = w_0 x_0 + w_1 x_1 + w_2 x_2$\n",
    "\n",
    "$z_A = 0*(-1) + (-1)*1 + 1*2 = 1$\n",
    "Since $z>0 \\Rightarrow y = 1$\n",
    "A is correctly classified and the weights are not updated.\n",
    "\n",
    "$z_B = 0*(-1) + (-1)*2 + 1*1 = -1$\n",
    "Since $z<0 \\Rightarrow y = 0$\n",
    "\n",
    "The weights will be updated: $\\vec{w} -= \\eta(y-t)\\vec{x} = (0,-1,1) - 0.1*(0-1)*(-1, 2, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abed6d2f-cc63-4652-bd03-2cc3682e2fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  2  1]\n",
      "[ 0.1 -0.2 -0.1]\n",
      "[-0.1 -0.8  1.1]\n"
     ]
    }
   ],
   "source": [
    "w = np.array((0,-1,1)); eta = 0.1\n",
    "y = 0; t = 1\n",
    "x_B = np.concatenate([np.array(([-1])), x[1,:]])\n",
    "print(x_B)\n",
    "print(eta*(y-t)*x_B)\n",
    "w = w - eta*(y-t)*x_B\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb1dc4-b412-41a1-9eae-17b036d87564",
   "metadata": {},
   "source": [
    "### MLP and back-propagation (10p)\n",
    "a) In order to use the MLP for regression instead of classification we need to change some of the steps in the algorithm. We need to change eq. 4.7 (last in forward phase) and eq. 4.8(first in backward phase). The change is as follows:\n",
    "\n",
    "eq. 4.7 $y_\\kappa = g(h_\\kappa) = \\frac{1}{1+\\exp (-\\beta h_\\kappa)} \\rightarrow y_\\kappa = g(h_\\kappa) = h_\\kappa$\n",
    "Simply not using the activation.\n",
    "\n",
    "eq. 4.8 $\\delta_o(\\kappa) = (y_\\kappa - t_\\kappa)y_\\kappa (1-y_\\kappa) \\rightarrow \\delta_o(\\kappa) = (y_\\kappa - t_\\kappa)$\n",
    "Simply having the $\\delta_o$ as the normal error (computed - expected)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8caddd-dd2e-4ddd-ac0f-9156b94c28e8",
   "metadata": {},
   "source": [
    "b) Marsland's algorithm uses the logistic(sigmoid) activation function in the hidden layer. In order to change to the RELU activation function we need to change eq's. 4.5 and 4.9:\n",
    "\n",
    "eq. 4.5 $a_\\zeta = g(h_\\zeta) = \\frac{1}{1+\\exp (-\\beta h_\\zeta)} \\rightarrow a_\\zeta = g(h_\\zeta) = RELU(h_\\zeta) = max(h_\\zeta, 0) $.\n",
    "\n",
    "eq. 4.9: $\\delta_h(\\zeta) = a_\\zeta(1-a_\\zeta) \\sum\\limits_{k=1}^N w_\\kappa \\delta_o(k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc38ca-76c3-456f-86c2-87de08d36532",
   "metadata": {},
   "source": [
    "### Majority voting classifier(8p)\n",
    "Majority voting classifying is simply to train multiple classifiers on the same problem and data and use the majority ruling classify the points. The circled point in the description is below two of the lines and above one. There is therefore a two against one vote to classify the points as class 0(red). The majority voting classification line will be one where there is one classifier on each side(blue-green-orange). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed0a06-443e-4c93-ae60-a0258f42d68d",
   "metadata": {},
   "source": [
    "### Regularization (8p)\n",
    "In supervised machine learning we find a model with a set of weights. The goal of the training is to find the weights which best fit the training data ($\\textbf{X, t})$. To determine the best fit we introduce the loss function, $L$ and set the goal of determining the weights which minimize the loss function.\n",
    "\n",
    "$ \\hat{\\textbf{w}} = argmax( - L(\\textbf{X, t, w}))$\n",
    "\n",
    "Regularization is applied to avoid overfitting. The goal is to avoid putting to much weight on some features and instead get more even weight between the features. We replace the objective of minimizing L with minimizing $L + \\alpha||\\textbf{w}||^2$.\n",
    "Where $\\alpha$ is optimized experimentally for each problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c70492-f7da-42eb-986a-336bc3bb6bc3",
   "metadata": {},
   "source": [
    "### Unsupervised learning (8p)\n",
    "a) We are considering the two algorithm's used for dimensionality reduction (PCA and autoencoders). Can they be used to generate higher dimensional representations? \n",
    "\n",
    "PCA is limited by the number of orthogonal eigenvectors and is therefore unable to generate higher dimensional representations. The autoencoders can learn higher dimensional representations, but one might have to change the loss function. \n",
    "\n",
    "c)If someone wanted to reduce a data set from say 1000 samples in 20 dimensions to 10 representative samples(prototypes) one could use k-means to find 10 centroids and use those as prototypes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e430e-0bf7-446b-bb46-28c0c57966b9",
   "metadata": {},
   "source": [
    "### Evolutionary algorithms and reinforcement learning (13p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af2d30-8f81-4a1c-819d-56c29656b4f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baa857-02dc-46ed-9557-01629d68b4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
